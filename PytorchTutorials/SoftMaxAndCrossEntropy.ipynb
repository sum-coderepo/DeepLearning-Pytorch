{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/ np.sum(np.exp(x), axis = 0)\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6590, 0.2424, 0.0986])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim = 0)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometime Softmax is applied with cross entropy loss to measure the classification problems.\n",
    "Lower loss means better prediction\n",
    "\n",
    "$$ D(\\hat{Y},Y) = \\frac{-1}{N} . \\sum Y_i. log(\\hat{Y_i}) $$\n",
    "\n",
    "#### 1. Cross Entropy Loss for Binary classification\n",
    "$$loss = label * (-1) * log(pred) + (1 — label) * (-1) * log(1 — pred)$$\n",
    "#### Here “label” can be either 0 or 1 and “pred” can be a probability value between 0 to 1 — any real value. </br>\n",
    "The loss is a scalar value.</br>\n",
    "So, for label=1, the loss will be</br>\n",
    "$$ {loss\\_label\\_1} = label * (-1) * log(pred) + 0$$\n",
    "$$ loss\\_label\\_1 = — log(pred) $$\n",
    "for label=0, the loss will be\n",
    "$$loss\\_label\\_0 = 0 + (1 — label) * (-1) * log(1 — pred)$$\n",
    "$${loss\\_label\\_0} = -log(1-pred)$$\n",
    "\n",
    "\n",
    "#### 2. Softmax Cross Entropy Loss for Binary Classification\n",
    "$${softmax\\_logits} = softmax(logits)$$\n",
    "$${loss\\_softmax\\_cross} = label * (-1) * log{(softmax\\_logits)} + (1 — label) * (-1) * log(1 — {(softmax\\_logits)})$$\n",
    "Here, because the logits are softmaxed, they contain the probability of being a positive class.\n",
    "\n",
    "#### 3. Softmax Cross Entropy Loss for Multi Class Classification\n",
    "$${softmax\\_logits} = softmax(logits){loss\\_softmax\\_cross\\_multi} = sum(label * (-1) * log({softmax\\_logits}))$$\n",
    "Here, labels and logits both are a vector / single column array. E.g. for 10 classes it is [10,] array. The sum represents the sum across the dimension depicted by num_of_classes which is in this case last dims. So, the loss is a scalar value.\n",
    "\n",
    "#### 4. Weighted Softmax Cross Entropy Loss for Multi Class Classification\n",
    "$${softmax\\_logits} = softmax(logits){loss\\_softmax\\_cross\\_multi} = sum({cls\\_weight} * label * (-1) * log({softmax\\_logits}))$$\n",
    "Here, labels, logits and cls_weights all are having same shape — a vector / single column array.</br>\n",
    "E.g. for 10 classes it is [10,] array. The loss is a scalar value. The cls_weight has a separate weight for each class.\n",
    "\n",
    "\n",
    "#### 5. Sigmoid Cross Entropy Loss \n",
    "The sigmoid cross entropy is same as softmax cross entropy except for the fact that instead of softmax, we apply sigmoid function on logits before feeding them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss Numpy for good pred : 0.3567\n",
      "loss Numpy for good pred : 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss\n",
    "\n",
    "# One hot encoded vector\n",
    "Y = np.array([1,0,0])\n",
    "\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'loss Numpy for good pred : {l1:.4f}')\n",
    "print(f'loss Numpy for good pred : {l2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From torch\n",
    "*** nn.CrossEntropyLoss applies (nn.LogSoftmax + nn.NLLLoss(negative log likelihodd loss))***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "1.840616226196289\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "# nsamples * nclasses = 1 * 3\n",
    "Y = torch.tensor([0])\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad =  torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print (l1.item()) \n",
    "print (l2.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(predictions1)\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Crossentropy for NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1()\n",
    "        out = self.relu(out)\n",
    "        out = self.Linear2(out)\n",
    "        # No softmax at the end\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet2(input_size = 28*28, hidden_size = 5, num_classes = 3)\n",
    "criterion = nn.CrossEntropyLoss()  # Applies Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for Binary classification use:-\n",
    "***nn.BCELoss()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        slef.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # Only One output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1()\n",
    "        out = self.relu(out)\n",
    "        out = self.Linear2(out)\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        # No softmax at the end\n",
    "        return y_pred\n",
    "    \n",
    "model = NeuralNet2(input_size = 28*28, hidden_size = 5, num_classes = 3)\n",
    "criterion = nn.BCELoss()  # Applies Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
