{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled24.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNppU7eJONUtIxAe7TijZjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sum-coderepo/DeepLearning-Pytorch/blob/master/PytorchTutorials/Loss_backward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg-p6kiCQqYA"
      },
      "source": [
        "**Training a NN happens in two steps:**\n",
        "\n",
        "**Forward Propagation:** In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
        "\n",
        "**Backward Propagation:** In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWz7QvkgFjO"
      },
      "source": [
        "When you call **loss.backward()**, all it does is compute gradient of loss w.r.t all the parameters in loss that have **requires_grad = True** and store them in **parameter.grad** attribute for every parameter.\n",
        "\n",
        "**optimizer.step()** updates all the parameters based on **parameter.grad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxe4qdW4i8Bw"
      },
      "source": [
        "\n",
        "\n",
        "loss is computing the loss between two tensors with no relation to a network. How does **loss.backward()** know which network it needs to reference and compute **parameter.grad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Wi1ZFcjEVl"
      },
      "source": [
        "Let's say we defined a model: model, and loss function: criterion and we have the following sequence of steps:\n",
        "```\n",
        "pred = model(input)\n",
        "loss = criterion(pred, true_labels)\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFZO1Nu1md0n"
      },
      "source": [
        "pred will have an `grad_fn` attribute, that references a function that created it, and ties it back to the model. Therefore, **loss.backward()** will have information about the model it is working with.\n",
        "\n",
        "\n",
        "Try removing `grad_fn` attribute, for example with:\n",
        "`pred = pred.clone().detach()`\n",
        "\n",
        "Then the model gradients will be None and consequently weights will not get updated.\n",
        "\n",
        "And the optimizer is tied to the model because we pass model.parameters() when we create the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w-W-422jGi7"
      },
      "source": [
        "import torch\n",
        "x = torch.tensor([1., 2.], requires_grad=True)\n",
        "y = 100*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoT4u29MgIPe",
        "outputId": "8e62f39a-9c1b-4732-a182-8ffe37e1c095"
      },
      "source": [
        "print(x,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2.], requires_grad=True) tensor([100., 200.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSDctEuwnR__"
      },
      "source": [
        "loss = y.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wan4gxGYi7VL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXPmgETfnU5T",
        "outputId": "3c760ee4-b1f5-45ff-9949-6a031d85962a"
      },
      "source": [
        "# Compute gradients of the parameters w.r.t. the loss\n",
        "print(x.grad)     # None\n",
        "loss.backward()      \n",
        "print(x.grad)     # tensor([100., 100.])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([100., 100.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMQ7WbE-nVkt",
        "outputId": "5089fd1d-9b76-4f62-db62-7a92c0332dc1"
      },
      "source": [
        "# MOdify the parameters by subtracting the gradient\n",
        "optim = torch.optim.SGD([x], lr=0.001)\n",
        "print(x)        # tensor([1., 2.], requires_grad=True)\n",
        "optim.step()\n",
        "print(x)        # tensor([0.9000, 1.9000], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2.], requires_grad=True)\n",
            "tensor([0.9000, 1.9000], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueJX9u3KnhsY"
      },
      "source": [
        "`loss.backward() `sets the grad attribute of all tensors with requires_grad=True in the computational graph of which loss is the leaf (only x in this case).</br>\n",
        "Optimizer just iterates through the list of parameters (tensors) it received on initialization and everywhere where a tensor has `requires_grad=True`, it subtracts the value of its gradient stored in its **.grad** property (simply multiplied by the learning rate in case of SGD). It doesn't need to know with respect to what loss the gradients were computed it just wants to access that .grad property so it can do `x = x - lr * x.grad`\n",
        "\n",
        "Note that if we were doing this in a train loop we would call `optim.zero_grad()` because in each train step we want to compute new gradients - we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIm9PAFsn4yQ"
      },
      "source": [
        "***Pytorch Example***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llcG2WMZnbwv"
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkWe19A3n_9e"
      },
      "source": [
        "Q = 3*a**3 - b**2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w2dGdGHVFAM"
      },
      "source": [
        "When we call `.backward()` on Q, autograd calculates these gradients and stores them in the respective tensorsâ€™ `.grad `attribute.\n",
        "\n",
        "We need to explicitly pass a gradient argument in `Q.backward()` because it is a vector. gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t. itself, i.e.\n",
        "\n",
        "$\\frac{dQ}{dQ} = 1$\n",
        "\n",
        "Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like `Q.sum().backward().`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2WIWsarNoD"
      },
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCCIqvtXVkaL"
      },
      "source": [
        "Gradients are now deposited in a.grad and b.grad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufSL9AC9OA0T",
        "outputId": "fdc94a90-9405-4580-9956-e0c5c3aa2dc3"
      },
      "source": [
        "a.grad, 9*a**2"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([36., 81.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHj6M8vgVyB_",
        "outputId": "883fe170-6469-4463-f511-54fd1045146b"
      },
      "source": [
        "-2*b, b.grad"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-12.,  -8.], grad_fn=<MulBackward0>), tensor([-12.,  -8.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOyU98GmV3HJ",
        "outputId": "7fc7845d-5acc-445c-c501-6a36498ca281"
      },
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GPsfyO6V6wJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}